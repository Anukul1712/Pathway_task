# Ollama Configuration
OLLAMA_BASE_URL=http://host.docker.internal:11434
OLLAMA_MODEL=llama3.2:3b

# Available models you can use:
# - llama3.2:1b (fastest, smallest)
# - llama3.2:3b (balanced)
# - llama3.1:8b (more capable)
# - mistral:7b
# - phi3:mini

# Pathway Configuration
PATHWAY_HOST=0.0.0.0
PATHWAY_PORT=8080

# Logging
LOG_LEVEL=INFO
